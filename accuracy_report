# Accuracy Evaluation: Rule-Based vs. Machine Learning

## Objective
Compare a traditional **rule-based system** with a **machine learning model (TF-IDF + Logistic Regression)** for extracting structured information from log messages. The goal was to measure accuracy and evaluate trade-offs between interpretability and generalization.

---

## What We Did
- **Defined slots**: `Rule`, `Time`, `User`, `Source`
- **Rule-based system**: Hand-crafted patterns, iteratively tuned
- **ML model**: TF-IDF features + Logistic Regression classifier
- **Evaluation**: Per-slot accuracy and exact match (all slots correct)

---

## Why We Did It
- Rules are quick to implement and interpretable but brittle (new phrasing = new rule).
- ML reduces manual effort and handles ambiguity/unseen phrasing.
- Comparing both gives stakeholders a clear trade-off analysis.

---

## Results

| Slot        | Rule-Based Accuracy | ML Accuracy |
|-------------|---------------------|-------------|
| Rule        | ~95%                | 94%         |
| Time        | ~91%                | 98%         |
| User        | ~100%               | 99%         |
| Source      | ~97%                | 95%         |
| **Exact Match** | ~90%            | ~95%        |

---

## Key Observations
- **Rule vs. ML parity**: Similar results on `Rule` and `User`.
- **ML advantage**: Significant boost on `Time` (handles “since yesterday / past day / last 24h”).
- **Rule edge**: Slightly better on `Source` (overlap between “auth” and “host”).
- **Overall**: ML pushes past the ~90% ceiling of rules, reaching ~95–98%.

---

## Interpretation
- **Rules**: Great baseline, interpretable, but brittle.
- **ML**: Generalizes well, reduces maintenance overhead.
- **Hybrid approach**: Rules for easy cases, ML for ambiguity.

---

## Takeaway
This project demonstrates:
- Transition from rules → ML in an extraction pipeline
- **Measured improvements**: 90% → 95–98%
- **Practical hybrid strategy** for real-world deployment

